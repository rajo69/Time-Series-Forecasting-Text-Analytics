---
title: "Assessed Coursework"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Part 1
## 1. Introduction

The problem given for first part of the assignment involves forecasting of personal consumption expenditures from a US seasonally adjusted data given in PCE.csv . As part of the assignment, the data is used across 3 different forecasting models as follows :

-   Simple forecasting method
-   Exponential smoothing method
-   ARIMA model

The forecasting accuracies of the models are discussed in the subsequent sections in details for evaluation, and the best model is used to find the personal expenditure estimate for October 2024. Further, one-step ahead rolling forecasting is performed for all 3 models without re-estimation of the parameters to find a relative comparison across the models. 

## 2. Data description and Preprocessing

On loading the given data, in a data-frame and inspecting it can be found out that the data set starts from *January 1959* and ends on *November 2023* giving a total of 779 readings for expenditures across 64 years.

```{r}
#Inspecting the data
data <- read.csv("PCE.csv")
head(data)
tail(data)
```

Now a time-series object is created using the start date and end date, and a periodicity of 12 months. On plotting the time-series it is seen that there is a **non-linear trend in the data** and it appears that the **personal expenditure values had increased almost exponentially over the years**. There is a sudden dip in the value during 2020 which can be attributed to ongoing global pandemic, other than which the overall trend seems non-linear over time, and already it has been mentioned that the given data is seasonally adjusted. Alongside the time-series shows presence of NULL values which need to be imputed before further analysis.

```{r}
# Plotting the time-series and looking for NULL values
ExpenseTS <- ts(data$PCE,start=c(1959, 1), end=c(2023,11), frequency=12)
plot(ExpenseTS, xlab = "Time (years)", ylab = "Expenditure", main = "US Personal Consumption Expenditure Over Time")
print(paste("Number of Null values : ",sum(is.na(ExpenseTS))))
```

### 2.1 Time-series Imputation 

While filling missing values in a time-series, it's essential to select an imputation method that can capture the complexity and dynamics of the data. The commonly used imputation techniques under *imputeTS* are as follows :

-   **na_interpolation** performs imputation of missing values in time series data using interpolation methods. This function supports several types of interpolation methods, including linear, spline, and stine interpolation. 
-   **na_ma** provides a straightforward way to handle missing values in time series data using moving average methods.
-   **na_kalman** provides a robust way to handle missing values in time series data using Kalman smoothing and state space models.

In this assignment the **interpolation function is used for imputation. The (option = "stine") is used as  Stineman interpolation can provide better estimates in cases where the data has a non-linear trend**.

```{r include=FALSE}
library(imputeTS)
# Stineman Interpolation on timeseries
ip_TS<- na_interpolation(ExpenseTS,option = "stine")
```

### 2.2 Decomposition and Analysis

Since the non-linear trend of the data is quite evident from the time-series plot, **multiplicative** decomposition is used to visualise the underlying components. The graph quite evidently points out that :

1. The trend of the data has an exponential incline with time.
2. The seasonality of the time-series is constant over time, since the data has been seasonally adjusted as already mentioned.
3. There is some finite fluctuations in the random component across time and the sudden dip in the data around 2020 is due to the underlying randomness of the global pandemic situation.

```{r include=FALSE}
library(forecast)
```

```{r}
#Multiplicative Decomposition
plot(decompose(ip_TS, type="multiplicative"))
```

Moving forward, the ACF plot is used to identify further information about the time-series. The **slow decay of the plot suggests that the trend component is quite strongly influencing the data** and should be taken care of while choosing a model, in order to effectively forecast it. Also it can be inferred that this time-series is a **non-stationary** one. 

```{r}
#ACF plot
acf(ip_TS, main = "ACF of Interpolated Time Series")
```


## 3. Train-test split

A **80:20** ratio has been used here for splitting the data. Hence out of the 64 years, first 51 years of observations are used for training the models and rest 13 years are used for testing (validation set). After comparing the accuracies the best found model is used on entire data-set to forecast the expenditures in subsequent 11 periods extending till October 2024.   

```{r echo=TRUE}
train_TS <- subset(ip_TS, end = (length(ip_TS) - (13*12)))
length(train_TS) # checking train set length
test_TS <- subset(ip_TS,start = (length(ip_TS) - (13*12) + 1))
length(test_TS) # checking validation set length
```

## 4. Forecasting using different models

### 4.1 Simple Forecasting Method - Drift 

Drift is one of the simple random walk forecasting methods used for making short-term forecasts. Here the forecasted value is calculated by taking in the last observation adjusted by the average change observed in the historical data, assuming that the time series will continue to change at the same average rate as per historical observations.It doesn't assume any underlying model structure, hence fails to capture any trend or seasonality. 

Below is a snippet of the Drift model accuracies on the test-data.

```{r}
# Drift forecasting
fc_drift<- rwf(train_TS,drift = TRUE,h=length(test_TS))
accuracy(fc_drift,test_TS)
```

### 4.2 Exponential Smoothing Method - Holt

Exponential smoothing methods produce forecasting results by giving more weightage to recent observations rather than older ones. In this case Holt's linear method is used for forecasting the expense time-series because there is trend present and simple exponential smoothing cannot account for underlying trend or seasonality in the data.

Below is a snippet of Holt's model accuracies on the test-data.

```{r}
# Holt forecasting
fc_holt <- holt(train_TS,h=length(test_TS))
accuracy(fc_holt,test_TS)
```

### 4.3 ARIMA model 

The AutoRegressive Integrated Moving Average (ARIMA) method is a statistical forecasting approach, where prediction of the time-series is done using a linear combination of its past values and errors. There is an underlying assumption that the data is stationery in this approach. The ARIMA model is denoted as ARIMA(p, d, q), where: 

- p: The number of autoregressive terms, represents the relationship between an observation and a number of lagged observations (autoregressive terms).
- d: The degree of differencing, indicating the number of times differencing is required to achieve stationarity.
- q: The number of moving average terms, representing the relationship between an observation and a residual error from a moving average model applied to lagged observations.

Here auto.arima() function is used to determine the ideal number of parameters for the train data. The model summary below shows the model parameters as (3,2,2).

```{r}
# ARIMA model
fc_ar<- auto.arima(train_TS)
fc_ar
```

Snippet below captures the ARIMA(3,2,2) model's accuracy on the test set.

```{r}
accuracy(forecast(fc_ar,h=length(test_TS)),test_TS)
```

## 5. Evaluation and Visualisation

For the evaluation of the model **Root Mean Squared Errors** and **Mean Absolute Errors** are taken into account. Below is a visualisation of the metrics for 3 models on test data : 



```{r}

#Visualising the error metrics

library(ggplot2)
library(reshape2)

models <- c("Drift", "Holt", "ARIMA")
RMSE <- c(2545.27790, 1151.49047, 1592.717)  
MAE <- c(1926.57567, 649.7501, 1042.39499)      

error_df <- data.frame(models, RMSE, MAE)

error_melted <- melt(error_df, id.vars = "models")

ggplot(error_melted, aes(x = models, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.5) +
  geom_text(aes(label = round(value, 2)), position = position_dodge(width = 0.5), vjust = -0.5) +
  labs(title = "Comparison of Forecasting Models", x = "Model", y = "Error") +
  scale_fill_manual(values = c("RMSE" = "blue", "MAE" = "red")) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(t = 10, r = 10, b = 10, l = 10, unit = "pt")
  )
```

It is visibly clear from the plots that *Holt model is performing better for the test data*. Plotting the prediction on the test window makes that further clear. 

```{r warning=FALSE}
#Plotting all 3 forecasts along side original data

Drift <- fc_drift$mean
Holt <- fc_holt$mean
ARIMA <- forecast(fc_ar,h=length(test_TS))$mean

autoplot(ip_TS) +
  autolayer(Drift, series = "Drift", PI = FALSE) +
  autolayer(Holt, series = "Holt", PI = FALSE) +
  autolayer(ARIMA, series = "ARIMA", PI = FALSE) +
  ggtitle("Forecasting with Drift, Holt, and ARIMA Models") +
  xlab("Year") + ylab("Personal Consumption Expense") +
  scale_color_manual(values = c("Drift" = "red", "Holt" = "blue", "ARIMA" = "green")) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(t = 10, r = 10, b = 10, l = 10, unit = "pt")
  )
```

It is safe to conclude from the above plots that ***exponential smoothing using Holt model performs best on the given data*** and should be used for estimating the expense for October 2024. While inferring the performance of the models following points must be considered :

- The Drift model as discussed earlier is a simple model and fails to capture any trend or seasonality in the data. Hence it performs very poorly on the forecasting window.
- The ARIMA model assumes that dataset is stationary. In this case the ACF plot suggests that this data is non-stationary hence ARIMA doesn't work too well.
- The exponential model using Holt's method is able to capture trend in a data when there is no seasonality and hence performs best on the model.

**The Expense estimate mean for October 2024 is 19566.92 USD**. Below is given the entire forecast window predictions along with intervals.

```{r}
#Forecast for October 2024
fc_final <- holt(ip_TS,h=11)
fc_final
```

## 6. One-step ahead rolling forecasting without re-estimation of the parameters

Rolling forecasts are an effective way of comparing forecasting models on a single set of training data. For this task, the existing models are refitted on the entire expense data-set and then the rolling forecasts starting December 2010 (as per train-test split) are compared with the test data for accuracy. The rolling forecasts are calculated using *fitted() function from fpp package*

```{r include=FALSE}
#Installing and loading fpp package
if(!require(fpp)) install.packages("fpp",repos = "http://cran.us.r-project.org")
library(fpp)
```

Below the accuracies of the one-step ahead forecasts are calculated on the existing models.

-   Drift Model Accuracies :

```{r}
#rolling drift model
refit_drift <- rwf(ip_TS,drift = TRUE,model=fc_drift)
fc_drift_roll <- window(fitted(refit_drift), start = c(2010,12) )
accuracy(fc_drift_roll,test_TS)
```

-   Holt Model Accuracies :

```{r}
#rolling holt model
refit_holt <- holt(ip_TS,model=fc_holt)
fc_holt_roll <- window(fitted(refit_holt), start = c(2010,12))
accuracy(fc_holt_roll,test_TS)
```

-   ARIMA Model Accuracies :

```{r}
#rolling (3,2,2) ARIMA model
refit_ar <- Arima(ip_TS, model=fc_ar)
fc_ar_roll <- window(fitted(refit_ar), start=c(2010,12))
accuracy(fc_ar_roll,test_TS)
```

The above results consistently resonates the fact that the Holt model is performing superiorly on the given data.

## 7. Conclusion

In this task 3 different models have been used to forecast predictions for a seasonally adjusted US personal expenditure data and it has been shown that **Holt exponential smoothing works best** on the given data. As found in course of the analysis the *given data has non-linear trend, is seasonally adjusted and is non-stationary*. These attributes makes it difficult for Drift and ARIMA models to work well while predicting the forecast windows. The model evaluations using both RMSE and MAE scores hold true for this inference. So it is safe to conclude that estimates made for October 2024 using the Holt model should be closest possible to actual value coming in future.

# Part 2

## 1.Introduction

For the second task, a set of online Hotel reviews have been given along with their respective ratings on a range of 1 - 5 with 1 denoting Low Satisfaction and 5 denoting High satisfaction. As part of the task, a text analysis model has to be designed to identify the factors that are discussed in positive and negative reviews respectively. In the subsequent sections, the notion for deciding positive and negative reviews, the steps for carrying out the analysis, and the criterion to decide number of topics has been discussed in details. Finally the topics have been labelled, in order to identify the top factors that affect customer satisfaction or grievances.

## 2.Data Preprocessing 

```{r include=FALSE}
library(dplyr) # basic data manipulation
library(tm) # package for text mining package
library(stringr) # package for dealing with strings
library(RColorBrewer)# package to get special theme color
library(wordcloud) # package to create wordcloud
library(topicmodels) # package for topic modelling
library(ggplot2) # basic data visualization
library(LDAvis) # LDA specific visualization 
library(servr) # interactive support for LDA visualization
library(textcat)
```

The entire data-set of 10000 hotel reviews in HotelsData.csv is loaded into a data-frame, and eventually following steps are executed for preparing the data :

- All non-English reviews are removed using *textcat library*
- For understanding the sentiment of the reviews the given score is referenced. **Reviews having score greater than 3 are considered for positive feedback and reviews having score less than 3 are considered negative. Hence reviews scoring 3 are considered neutral**.
- After dropping the neutral reviews and NULL value rows, sampling is done from the remnant data using the method mentioned in brief.
- The sample is then divided in to positive and negative sets for next steps of analysis and topic modelling. (Single examples of positive and negative reviews given below)

```{r}
#Removing non-english reviews
reviews <- read.csv(file = "HotelsData.csv", header = TRUE)
reviews$language <- textcat(reviews$Text.1)
reviews_english <- reviews[reviews$language == "english", ]
#Dropping Neutral Reviews and NULL values
reviews_final <- reviews_english[reviews_english$Review.score != 3 ,]
reviews_final <- na.omit(reviews_final)
#Data Sampling
set.seed(430)
test<-sample_n(reviews_final, 2000)
test <- subset(test, select = -language)
pos_set <- test[test$Review.score > 3,]
neg_set <- test[test$Review.score < 3,]
#Inspecting the reviews
head(pos_set,1)
head(neg_set,1)
```

## 3.Creating Document Term Matrix with Term Frequency and Word Cloud Visualisation

After separating out the Positive and Negative reviews, it is seen that there are around 1760 and 240 counts in the respective data-sets. Next the term frequency document term matrix is created in order to be used as an input to learn which words are frequently found together in a document so that it could try to model the topics. The steps are broken down in subsequent points.

1. Two respective corpuses are created from the review datasets after *converting the document contents to utf-8 encoding as some of the characters in the text are not characters that tm package can handle*.

```{r}
#Creating Corpus
pos_reviews <- stringr::str_conv(pos_set$Text.1, "UTF-8")
pos_docs <- Corpus(VectorSource(pos_reviews))
neg_reviews <- stringr::str_conv(neg_set$Text.1, "UTF-8")
neg_docs <- Corpus(VectorSource(neg_reviews))
```


2. Next the **DocumentTermMatrix() function** is used for forming the matrix from the text data and it implicitly takes care of data cleaning steps like lemmatization, along with removal of punctuation, numbers, stopwords, and finally lowercase all tokens.


```{r}
# Creating DTM 
pos_dtmdocs <- DocumentTermMatrix(pos_docs,
                              control = list(lemma=TRUE,removePunctuation = TRUE,
                                             removeNumbers = TRUE, stopwords = TRUE,
                                             tolower = TRUE))
pos_raw.sum=apply(pos_dtmdocs,1,FUN=sum)
pos_dtmdocs=pos_dtmdocs[pos_raw.sum!=0,]
neg_dtmdocs <- DocumentTermMatrix(neg_docs,
                              control = list(lemma=TRUE,removePunctuation = TRUE,
                                             removeNumbers = TRUE, stopwords = TRUE,
                                             tolower = TRUE))
neg_raw.sum=apply(neg_dtmdocs,1,FUN=sum)
neg_dtmdocs=neg_dtmdocs[neg_raw.sum!=0,]

```

3. The output of the DTM function is then converted to a matrix for finding frequency of the top most used words and using those frequencies are used in plotting the word-clouds. Below the top 10 used words in both positive and negative review corpuses are shown followed with respective wordclouds.

```{r}
#Creating new matrix to find word frequency and visualise Word cloud
library(wordcloud)
pos_dtm.new <- as.matrix(pos_dtmdocs)
pos_frequency <- colSums(pos_dtm.new)
pos_frequency <- sort(pos_frequency, decreasing=TRUE)
pos_doc_length <- rowSums(pos_dtm.new)

pos_frequency[1:10]

pos_words <- names(pos_frequency)

wordcloud(pos_words[1:100], pos_frequency[1:100], rot.per=0.15, 
          random.order = FALSE, scale=c(4,0.5),
          random.color = FALSE, colors=brewer.pal(8,"Dark2"))
title(main = "Positive review wordcloud")
```

```{r}
neg_dtm.new <- as.matrix(neg_dtmdocs)
neg_frequency <- colSums(neg_dtm.new)
neg_frequency <- sort(neg_frequency, decreasing=TRUE)
neg_doc_length <- rowSums(neg_dtm.new)

neg_frequency[1:10]

neg_words <- names(neg_frequency)

wordcloud(neg_words[1:100], neg_frequency[1:100], rot.per=0.15, 
          random.order = FALSE, scale=c(4,0.5),
          random.color = FALSE, colors=brewer.pal(8,"Dark2"))
title(main = "Negative review wordcloud")
```

## 4. Topic Modelling with Latent Dirichlet Allocation (LDA)

Topic modelling uses a probability mapping function to determine if a particular word is correlated to a certain topic using the co-occurrence of words in the documents. In R **ldatuning and topicmodels libraries** are used for this purpose. There are 2 major steps in this.

### 4.1 Determining the number of topics

Using the ldatuning library, the optimal number of topics(k) are decided for LDA to generate out of the review set. In this case three criterions are choosen out of the available options in the library. The code **minimizes the criteria Arun2010 and CaoJuan2009 and maximizes the Griffiths2004 over a range of 5 - 20 possible topics and the optimal number of topics is decided by graphical inspection**.

-   For the **positive review DTM, 14 is choosen as the optimal number of topics**. As seen in the plots below, Griffiths2004 reaches an elbow maxima at k=14, and Arun2010 is not converging beyond k=13. So increasing topics beyond this point leads to concentric topic segregation.

```{r warning=FALSE}
#Determining number of topics
library(ldatuning)
pos_result <- FindTopicsNumber(
  pos_dtm.new,
  topics = seq(from = 5, to = 20, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
  method = "Gibbs",
  control = list(seed = 430),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(pos_result)
```

-   For the **negative review DTM, 11 is choosen as the optimal number of topics**. In the plots, at k = 11, CaoJuan2009 seems to have achieved global minima and Griffiths2004 achieves a converging elbow and doesn't show any major maximisation trend beyond this point. 

```{r warning=FALSE}
neg_result <- FindTopicsNumber(
  neg_dtm.new,
  topics = seq(from = 5, to = 20, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
  method = "Gibbs",
  control = list(seed = 430),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(neg_result)
```

### 4.2 Modelling with LDA 

This step involves generating a list of the topics covered by the documents and of grouping documents by the topics that was found. LDA function () is used with 1000 iterations for both the corpuses in this case, and it takes in the number of topics(k) decided in previous step as input.

```{r}
#Topic modelling
library(dplyr)
pos_ldaOut <-LDA(pos_dtmdocs,14, method="Gibbs", 
                 control=list(iter=1000,seed=430))
neg_ldaOut <-LDA(neg_dtmdocs,11, method="Gibbs", 
                 control=list(iter=1000,seed=430))
```

## 5. Topic Labelling

For labelling the topics, the command term() is used with the output of LDA and the top 10 terms for each topic is inspected. The labels are based on the predominant themes conveyed by those top 10 words.

### 5.1 Positive topics labelling 

Referring to the top 10 terms in each topic, the labels are as follows:

1.    Staff and Service quality
2.    Business travel in London
3.    Overall Guest Impression
4.    Room and Bathroom Facilities
5.    Recommendations based on Overall Satisfaction
6.    Food and Cleanliness
7.    Complimentary Amenities
8.    Check-in Process and Guest Experiences
9.    Stay Experience
10.   Hotel Surroundings
11.   Stay Duration
12.   Accessibility and Proximity to Transportation
13.   Location and size of Hotel
14.   Exceptional Hospitality

```{r}
pos_ldaOut.terms <- as.matrix(terms(pos_ldaOut, 10))
pos_ldaOut.terms
```


### 5.2 Negative topics labelling 

Referring to the top 10 terms in each topic, the labels are as follows:

1.    Overall Hotel Issues
2.    Breakfast and Location problems
3.    Value for Money
4.    Cleanliness and Maintenance
5.    Ventilation and temperature complaints
6.    London Hotel Experiences
7.    Reception and Staff Issues
8.    Room Comfort and Bar Issues
9.    Bathroom and Shower Issues
10.   Overall Stay Experience
11.   Staff and Service Quality

```{r}
neg_ldaOut.terms <- as.matrix(terms(neg_ldaOut, 10))
neg_ldaOut.terms
```

From the above labels, it can be inferred that *some of the topics are interrelated*, but mostly **reviews are concerned around overall service quality, stay experiences, location and cleanliness of the hotels**. To determine the top factors governing the nature of review, further analysis is done in next section.

## 6. Top factors affecting customer sentiments

For determining the factors that affect customer satisfaction and grievances, the original reviews are assigned under the topics modeled in previous section. Then these topic label counts are compared to understand the relevance of those factors across the review set and determine top 3 factors for both positive and negative reviews.

### 6.1 Top 3 factors in Positive reviews

As visualised in the countplot below, the topics most discussed in satisfactory reviews are :

1. **Topic 12 : Accessibility and Proximity to Transportation**
2. **Topic 1 : Staff and Service quality**
3. **Topic 14 : Exceptional Hospitality**

```{r}
library(ggplot2)
pos_ldaOut.topics <- data.frame(topics(pos_ldaOut))
pos_ldaOut.topics$index <- as.numeric(row.names(pos_ldaOut.topics))
ggplot(pos_ldaOut.topics, aes(x = topics.pos_ldaOut.)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Count Plot for Positive Review topics", x = "Topic", y = "Count") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 15, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
```

### 6.2 Top 3 factors in Negative reviews

As visualised in the countplot below, the topics most discussed in unpleasant reviews are :

1. **Topic 2 : Breakfast and Location problems**
2. **Topic 6 : London Hotel Experiences**
3. **Topic 9 : Bathroom and Shower Issues**

```{r}
neg_ldaOut.topics <- data.frame(topics(neg_ldaOut))
neg_ldaOut.topics$index <- as.numeric(row.names(neg_ldaOut.topics))
ggplot(neg_ldaOut.topics, aes(x = topics.neg_ldaOut.))+
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Count Plot for Negative Review topics", x = "Topic", y = "Count") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 15, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
```

## 7. Conclusion

In course of this text analysis task, a variety of transformations has been carried on the online hotel review data to understand the factors affecting customer satisfaction. While the words "hotels" and "rooms" were most used in terms of frequency within the token corpus, the broader subject of discussion across the reviews were found to be different. **Across both the positive and negative review sets, Accessibility to transport and Location has been a common key topic of interest, along side overall quality of service and cleanliness**. It can be concluded that Location of a hotel, and the kind of hospitality offered to the guests are the most prominent factors that has been captured in the online reviews.





